
Spring 2023

ì¼ë‹¨ reading assignmentì— ëª…ì‹œëœ ì±•í„°ë§Œ ì½ì–´ë³¸ë‹¤.

êµì¬ëŠ” Computer Organization and Design RISC-V Edition: The Hardware/Software Interfaceâ€ (COD) David A. Patterson, John L. Hennessy 2nd edition, Morgan Kaufmann, 2020 ISBN-13: 978-0-12-820331-6

```txt
Makefile ì°¸ê³ 

CC=/opt/riscv/bin/riscv64-unknown-elf-gcc
ABI=lp64d
ARCH=rv64g
CFLAGS=-mabi=$(ABI) -march=$(ARCH) -O1 -S

riscv64-unknown-elf-gcc -mabi=lp64d -march=rv64g -O1 -S
```

XORì´ SUB ë³´ë‹¤ ë™ë“±ì„± ê²€ì‚¬ë¥¼ ë¹ ë¥´ê³  ì ì€ ì „ë ¥ìœ¼ë¡œ í•  ìˆ˜ë„ ìˆë‹¤.

Design Principle 1: Simplicity favors regularity.

Design Principle 2: Smaller is faster.

Design Principle 3: Good design demands good compromises.

## Course Organization

Computer architecture is a **specification detailing how a set of software and hardware technology standards interact to form a computer system or platform**. In short, computer architecture refers to how a computer system is designed and what technologies it is compatible with.

ì»´í“¨í„°ê°€ ì–´ë–»ê²Œ controlë˜ê³  buildë˜ëŠ”ì§€ë¥¼ ë°°ìš´ë‹¤.

Computer Science is all about Abstractions.

Modern Computer Architecture is about managing and optimizing across several levels of abstraction w.r.t. dramatically changing technology and application load

## 1. Computer Abstractions and Technology

1-8, 11-12

> Civilization advances by extending the number of important operations which we can perform without thinking about them.

### 1. Introduction

> Had the transportation industry kept pace with the computer industry, for example, today we could travel from New York to London in a second for a penny.

**terabyte (TB)** Originally 1,099,511,627,776 (2^40) bytes, although communications and secondary storage systems developers started using the term to mean 1,000,000,000,000 (10^12) bytes. To reduce confusion, we now use the term tebibyte (TiB) for 2^40 bytes, defining terabyte (TB) to mean 10^12 bytes.

In consumer-oriented embedded applications, such as a digital home appliance, **dependability** is achieved primarily through simplicityâ€” the emphasis is on doing one function as perfectly as possible. In large embedded systems, techniques of redundancy from the server world are often employed.

40ë…„ ì „ì— PCì˜ ì‹œëŒ€ê°€ ì˜¨ ê²ƒì²˜ëŸ¼ ìµœê·¼ì—ëŠ” PMD(personal mobile device)ë¡œ ë°”ë€Œê³  ìˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ conventional serverì—ì„œ cloud computingìœ¼ë¡œ ë°”ë€Œê³  ìˆë‹¤. Todayâ€™s software developers will often have a portion of their application that runs on the PMD and a portion that runs in the Cloud. Todayâ€™s software developers will often have a portion of their application that runs on the PMD and a portion that runs in the Cloud.

Programmers interested in performance now need to understand the issues that have replaced the simple memory model of the 1960s: the parallel nature of processors and the hierarchical nature of memories. ... Moreover, as we explain in Section 1.7, todayâ€™s programmers need to worry about energy efficiency of their programs running either on the PMD or in the Cloud, which also requires understanding what is below your code.

This table summarizes how the hardware and software affect performance.

- Algorithm: Determines both the number of source-level statements and the number of I/O operations executed
- Programming language, compiler, and architecture: Determines the number of computer instructions for each source-level statement
- Processor and memory system: Determines how fast instructions can be executed
- I/O system (hardware and operating system): Determines how fast I/O operations may be executed

### 2. Seven Great Ideas in Computer Architecture

- Use Abstraction to Simplify Design
- Make the Common Case Fast
- Performance via Parallelism
- Performance via Prediction
- Hierarchy of Memories
- Dependability via Redunduncy

### 3. Below Your Program

Hardware > Systems software(**os, compiler**) > Application software

ëª…ë ¹ì–´ì™€ ë°ì´í„° ëª¨ë‘ë¥¼ ìˆ˜ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì€ ì»´í“¨íŒ…ì˜ ê¸°ì´ˆì´ë‹¤.

> **assembler**: A program that translates a symbolic version of instructions into the binary version.

> **assembly language**: A symbolic representation of machine instructions.

> **high-level programming language**: A portable language such as C, C++, Java, or Visual Basic that is composed of words and algebric notation that can be translated by a compier into assembly language.

```c
void swap(int v[], int k) {
    int temp;
    temp = v[k];
    v[k] = v[k + 1];
    v[k + 1] = temp;
}
```

!swap.s@!

í”„ë¡œê·¸ë˜ë° ì–¸ì–´ëŠ” ê°œë°œì¤‘ì¸ ì»´í“¨í„°ì— ë…ë¦½ì ì¸ í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤. ì»´íŒŒì¼ëŸ¬ì™€ ì–´ì…ˆë¸”ëŸ¬ê°€ ì„ì˜ì˜ ì»´í“¨í„°ë¥¼ ìœ„í•œ ì´ì§„ ëª…ë ¹ì–´ë¡œ ë²ˆì—­í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.

### 4. Under the Covers

ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ì‚´í´ë´¤ê³  ì´ì œ í•˜ë“œì›¨ì–´ë¥¼ ì‚´í´ë³´ì.

ëª¨ë“  ì»´í“¨í„°ì˜ ê¸°ë°˜ í•˜ë“œì›¨ì–´ì—ëŠ” inputting data, outputting data, processing data, storing dataì˜ ê¸°ë³¸ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•œë‹¤.

ì±…ì—ì„œ Big PictureëŠ” ì¤‘ìš”í•˜ë‹¤!

**The BIG Picture**: The five classic components of a computer are input, output, memory, datapath, and control, with the last two sometimes combined and called the processor

The image is composed of a matrix of picture elements, or pixels, which can be presented as a matrix of bits, called a **bit map**. ... The computer hardware support for graphics consists mainly of a **raster refresh buffer**, or **frame buffer**, to store the bit map.

> **integrated circuit**: Also called a **chip**. A device combining dozens to millions of transistors.

> **central processor unit (CPU)**: Also called processor. The active part of the computer, which contains the datapath and control and which adds numbers, tests numbers, signals I/O devices to activate, and so on.

í”„ë¡œì„¸ì„œëŠ” datapathì™€ controlë¡œ êµ¬ì„±ëœë‹¤. ê°ê° ê·¼ìœ¡ê³¼ ë‡Œì— ë¹„ìœ ë¨. ì „ìëŠ” ì‚°ìˆ  ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³ , í›„ìëŠ” ëª…ë ¹ì–´ë¥¼ ë”°ë¼ datapath, memory, I/O deviceì— ì§€ì‹œí•œë‹¤.

RAMì€ ë©”ëª¨ë¦¬ì˜ ë¶€ë¶„ì— ìƒê´€ì—†ì´ ì½ëŠ”ë° ê°™ì€ ì‹œê°„ì´ ì†Œìš”ëœë‹¤.

> **instruction set architecture**(Also called architecture): An abstract interface between the hardware and the lowest-level software that encompasses all the information necessary to write a machine language program that will run correctly, including instructions, registers, memory access, I/O, and so on.

> **application binary interface (ABI)**: The user portion of the instruction set plus the operating system interfaces used by application programmers. It defines a standard for binary portability across computers.

**The BIG Picture**: Both hardware and software consist of hierarchical layers using abstraction, with each lower layer hiding details from the level above. One key interface between the levels of abstraction is the instruction set architectureâ€”the interface between the hardware and low-level software. This abstract interface enables many implementations of varying cost and performance to run identical software.

íœ˜ë°œì„± ë©”ëª¨ë¦¬ - ë©”ì¸ ë©”ëª¨ë¦¬ í˜¹ì€ 1ì°¨ ë©”ëª¨ë¦¬. ë¹„íœ˜ë°œì„± ë©”ëª¨ë¦¬ - 2ì°¨ ë©”ëª¨ë¦¬. ì‚¬ì´ì¦ˆì™€ form factor ë•Œë¬¸ì— PMDì—ì„œëŠ” í”Œë˜ì‹œ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©. ê·¼ë° í”Œë˜ì‹œ ë©”ëª¨ë¦¬ëŠ” ì ì°¨ wear outë¨. í”Œë˜ì‹œ ë©”ëª¨ë¦¬ëŠ” ë¹„íœ˜ë°œì„±.

> **local area network (LAN)**: A network designed to carry data within a geographically confined area, typically within a single building. ì´ë”ë„·ì´ LANì˜ ì¼ì¢…ì¸ë“¯?

ë„¤íŠ¸ì›Œí¬ìª½ ë…¸ì¼,,

Check Yourself: DRAMë§Œ volatile. ì†ë„ì™€ ê°€ê²©ì€ DRAM > flash memory > disk storage. DRAM memory: volatile, short access time of 50 to 70 nanoseconds, and cost per GB is $5 to $10. Disk memory: nonvolatile, access times are 100,000 to 400,000 times slower than DRAM, and cost per GB is 100 times cheaper than DRAM. Flash memory: nonvolatile, access times are 100 to 1000 times slower than DRAM, and cost per GB is 7 to 10 times cheaper than DRAM.

### 5. Technologies for Building Processors and Memory

íŠ¸ëœì§€ìŠ¤í„°ëŠ” ì „ê¸°ì  ì‹ í˜¸ì— ì˜í•´ ì¡°ì ˆë˜ëŠ” ì˜¨ì˜¤í”„ ìŠ¤ìœ„ì¹˜. IC(integrated circuit)ì—ëŠ” ìˆ˜ë°±ê°œì˜ íŠ¸ëœì§€ìŠ¤í„°ê°€ ìˆë‹¤. VLSIëŠ” conductor + insulator + switchë“¤ì„ ì‹¤ë¦¬ì½˜ì— ë°°ì¹˜í•œ ê²ƒì´ë‹¤.

Silicon crystal ingot -> blank wafer -> patterened wafer -> tested wafer -> tested dies -> packaged dies -> tested packaged dies.

The simplest way to cope with imperfection is to place many independent components on a single wafer. The patterned wafer is then chopped up, or diced, into these components, called dies and more informally known as chips. Dicing ë•ë¶„ì— wafer ì „ì²´ê°€ ì•„ë‹Œ ê²°í•¨ì´ ìˆëŠ” ë¶€ë¶„ë§Œ ë²„ë¦´ ìˆ˜ ìˆë‹¤.

> **yield**: The percentage of good dies from the total number of dies on the wafer.

The cost of an integrated circuit rises quickly as the die size increases, due both to the lower yield and to the fewer dies that fit on a wafer. To reduce the cost, using the next generation process shrinks a large die as it uses smaller sizes for both transistors and wires. This improves the yield and the die count per wafer. A 7-nanometer (nm) process was state-of-the-art in 2020, which means essentially that the smallest feature size on the die is 7 nm.

This die uses a 10-nanometer technology, which means that the smallest features are approximately 10 nm in size, although they are typically somewhat smaller than the actual feature size, which refers to the size of the transistors as â€œdrawnâ€ versus the final manufactured size.

Cost per die = Cost per wafer / (Dies per wafer \* yield)

Dies per wafer = Wafer area / Die area

Yield = 1 / (1 + Defects per area \* Die area)^N

Nì€ number of critical processing step. defect rate, size of the die and waferë¡œ ê²°ì •ë˜ëŠ” costëŠ” die areaì— linearì— í•˜ì§€ ì•Šë‹¤.

**Check Yourself**: 2, 4??? 1, 3, and 4 are valid reasons. Answer 5 can be generally true because high volume can make the extra investment to reduce die size by, say, 10% a good economic decision, but it doesn't have to be true. ì•„ volumeì´ ìƒì‚°ëŸ‰ ëŠë‚Œì¸ê±´ê°€? ê·œëª¨ì˜ ê²½ì œ?

### 6. Performance

> **response time(execution time)**: The total time required for the computer to complete a task, including disk accesses, memory accessed, I/O activites, operating system overhead, CPU execution time, and so on.

> **throughput(bandwidth)**: Another measure of performance, it is the number of tasks completed per unit time.

response timeì€ ì¼ë°˜ PC ì‚¬ìš©ì, throughputì€ ë°ì´í„°ì„¼í„°ì—ì„œ ì‹ ê²½ì“¸ ë“¯í•œ ì§€í‘œì´ë‹¤.

ëŒ€ë¶€ë¶„ì˜ ê²½ìš° response timeì´ ì¤„ë©´ throughputì´ ì¦ê°€í•œë‹¤. ì—­ì€ ì• ë§¤í•˜ì§€ë§Œ ë§Œì•½ ì‘ì—…ì´ queue upë ì •ë„ë¡œ ë§ìœ¼ë©´ waiting time in queueë¥¼ ì¤„ì´ê¸°ì— ì—­ë„ ì„±ë¦½í•œë‹¤.

Performance = 1 / Execution time

> **wall clock time, response time, elapsed time**: Total time to complete a task, including disk accesses, memory accesses, I/O activities, os overhead etc.

í•˜ì§€ë§Œ í”„ë¡œì„¸ì„œëŠ” ì—¬ëŸ¬ í”„ë¡œê·¸ë¨ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ê³  ìˆì„ìˆ˜ë„ ìˆë‹¤. ì´ ê²½ìš° ì‹œìŠ¤í…œì€ í•œ í”„ë¡œê·¸ë¨ì˜ elapsed timeì„ ì¤„ì´ê¸°ë³´ë‹¤ throughputì„ ìµœì í™”í•˜ë ¤í•œë‹¤.

> ì´ëŸ¬í•œ ë§¥ë½ì—ì„œ **CPU execution time** í˜¹ì€ **CPU time**ì€ CPUê°€ ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ”ë° í•„ìš”í•œ ì‹œê°„ì´ë©° I/Oë‚˜ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì„ ê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„ì´ í¬í•¨ë˜ì–´ìˆì§€ ì•Šë‹¤. CPU timeì€ CPUê°€ í”„ë¡œê·¸ë¨ ìì²´ì— ì†Œìš”í•œ ì‹œê°„ì¸ **user CPU time**ê³¼ OSê°€ í”„ë¡œê·¸ë¨ì„ ìœ„í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì‹œê°„ì¸ **system CPU time**ìœ¼ë¡œ ë‚˜ë‰œë‹¤.

We will use the term system performance to refer to elapsed time on an unloaded system and CPU performance to refer to user CPU time.

> **clock cycle**(tick, clock tick, clock period, clock, cycle): The time for one clock period, usually of the processor clock, which runs at a constant rate.

> **clock rate**: Inverse of the clock period.

CPU time = CPU cycle for program \* Clock cycle time

CPU time = CPU cycle for program / Clock rate

ìœ„ ì‹ì€ instructionì˜ ê°œìˆ˜ë¥¼ í¬í•¨í•˜ì§€ ì•Šì§€ë§Œ ì–˜ë„ í¬í•¨ë˜ëŠ”ê²Œ ìì—°ìŠ¤ëŸ½ë‹¤.

CPU clock cycle = Instructions for a program \* Average clock cycles per instruction

> **clock cycles per instruction(CPI)**: Average number of clock cycles per instruction for a program or program fragment.

CPI provides one way of comparing two different implementations of the identical instruction set architecture, since the number of instructions executed for a program will, of course, be the same.

**The BIG Picture**: (CPU) Time = Seconds/Program = Instructions/Program \* Clock cycles/Instruction(CPI) \* Seconds/Clock cycle(cloc cycle time) ì„±ëŠ¥ì— ëŒ€í•œ ì˜¨ì „í•œ ì§€í‘œëŠ” ì‹œê°„ë¿ì´ë‹¤. ê³±ì…ˆì— ìˆëŠ” í•­ í•˜ë‚˜ë¥¼ ë¹¼ì„œ ê·¸ê²ƒìœ¼ë¡œ íŒë³„í•  ìˆ˜ëŠ” ì—†ìŒ.

Since the instruction count depends on the architecture, but not on the exact implementation, we can measure the instruction count without knowing all the details of the implementation. The CPI, however, depends on a wide variety of design details in the computer.

> **instruction mix**: A measure of the dynamic frequency of instructions across one or many programs.

CPIëŠ” instruction mixì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆê¸°ì— instruction countì™€ CPI ëª¨ë‘ ë¹„êµë˜ì–´ì•¼í•œë‹¤?

- Algorithm. Instruction countì™€ CPIì— ì˜í–¥ì„ ì¤€ë‹¤. The algorithm determines the number of source program instructions executed and hence the number of processor instructions executed. The algorithm may also affect the CPI, by favoring slower or faster instructions. For example, if the algorithm uses more divides, it will tend to have a higher CPI.
- Programming language. Instruction countì™€ CPIì— ì˜í–¥ì„ ì¤€ë‹¤. The programming language certainly affects the instruction count, since statements in the language are translated to processor instructions, which determine instruction count. The language may also affect the CPI because of its features; for example, a language with heavy support for data abstraction (e.g., Java) will require indirect calls, which will use higher CPI instructions.
- Compiler. Instruction countì™€ CPIì— ì˜í–¥ì„ ì¤€ë‹¤. The efficiency of the compiler affects both the instruction count and average cycles per instruction, since the compiler determines the translation of the source language instructions into computer instructions. The compilerâ€™s role can be very complex and affect the CPI in varied ways.
- Instruction set architecture. Instruction count, clock rate, CPIì— ì˜í–¥ì„ ì¤€ë‹¤. The instruction set architecture affects all three aspects of CPU performance, since it affects the instructions needed for a function, the cost in cycles of each instruction, and the overall clock rate of the processor.

**Check Yourself**: 1.a ë‘˜ ë‹¤, 1.b response time, 1.c **neither**. 2. 7.

**Check Yourself**: 15\*0.6\*1.1=9.9

### 7. The Power Wall

The reason for power and clock rate's recent slowing is that we have run into the practical power limit for cooling commodity microprocessors.

Just as measuring time in seconds is a safer evaluation of program performance than a rate like MIPS (see Section 1.10), the energy metric joules is a better measure than a power rate like watts, which is just joules/second. ì‹¤í–‰ ì‹œê°„ì´ í¬í•¨ë˜ì–´ì„œ ê·¸ëŸ°ê°€?

CMOSì—ì„œ ì—ë„ˆì§€ ì†Œë¹„ëŠ” ì£¼ë¡œ dynamic energyì—ì„œ ì¼ì–´ë‚œë‹¤. íŠ¸ëœì§€ìŠ¤í„°ì˜ ìƒíƒœ(0, 1)ê°€ ë°”ë€” ë•Œ ì†Œë¹„ë˜ëŠ” ì—ë„ˆì§€. The dynamic energy depends on the capacitive loading of each transistor and the voltage applied: Energy âˆ Capacitive load \* Voltage^2. ì–˜ëŠ” 0->1->0ì—ì„œì˜ ì—ë„ˆì§€. The energy of a single transition is then Energy âˆ 1/2 \* Capacitive load \* Voltage^2.

The power required per transistor is just the product of energy of a transition and the frequency of transitions: Power âˆ 1/2 \* Capacitive load \* Voltage^2 \* Frequency switched.

Frequency switched is a function of the clock rate. The capacitive load per transistor is a function of both the number of transistors connected to an output (called the fanout) and the technology, which determines the capacitance of both wires and transistors.

With regard to Figure 1.16, how could clock rates grow by a factor of 1000 while power increased by only a factor of 30? Energy and thus power can be reduced by lowering the voltage, which occurred with each new generation of technology, and power is a function of the voltage squared. Typically, the voltage was reduced about 15% per generation. In 20 years, voltages have gone from 5 V to 1 V, which is why the increase in power is only 30 times.

The modern problem is that further lowering of the voltage appears to make the transistors too leaky, like water faucets that cannot be completely shut off. 40% ì „ë ¥ ì†Œë¹„ê°€ ì´ê±° ë•Œë¬¸.

Although dynamic energy is the primary source of energy consumption in CMOS, static energy consumption occurs because of leakage current that flows even when a transistor is off.

Power is a challenge for integrated circuits for two reasons. First, power must be brought in and distributed around the chip; ...Second, power is dissipated as heat and must be removed.

Power wallì— ë¶€ë”ªíˆë©´ì„œ ë‹¤ë¥¸ ë°©ë²•ì´ í•„ìš”í•˜ê²Œ ë˜ì—ˆë‹¤.

### 8. The Sea Change: The Switch from Uniprocessors to Multiprocessors

Response timeì„ ì¤„ì´ê¸°ë³´ë‹¨ throuputì„ ì¤„ì´ëŠ”ë° ì¢‹ì€ microprocessors with multiple processors per chipì— ì§‘ì¤‘í•˜ê²Œ ë˜ì—ˆë‹¤. ì°¸ê³ ë¡œ íšŒì‚¬ë“¤ì€ processorë¥¼ coreë¡œ, ì´ëŸ¬í•œ microsessorë“¤ì„ multicore microprocessorë¼ ë¶€ë¥¸ë‹¤. ë§ˆì´í¬ë¡œí•œê±° ì•ˆì— ê·¸ëƒ¥ í”„ë¡œì„¸ì„œê°€ ìˆëŠ”ê²Œ ì›ƒê¸°ë„¤,,

ì´ì œ ì´ëŸ¬í•œ ë§ˆì´í¬ë¡œí”„ë¡œì„¸ì„œë“¤ì˜ ì„±ëŠ¥ì„ ë½‘ì•„ë¨¹ìœ¼ë ¤ë©´ í”„ë¡œê·¸ë˜ë¨¸ë“¤ë„ ë©€í‹°í”„ë¡œì„¸ì„œë¥¼ ì´í•´í•´ì•¼í•œë‹¤.

ì˜ˆì „ì— í”„ë¡œê·¸ë˜ë¨¸ë“¤ì—ê²Œ parallel hardwareë¥¼ ì´í•´í•˜ê³  í”„ë¡œê·¸ë¨ ë‹¤ì‹œ ì§œë¼ê³  í–ˆì„ë•ŒëŠ” ì‹¤íŒ¨í–ˆëŠ”ë°, itâ€™s startling that the whole IT industry bet its future that programmers will successfully switch to explicitly parallel programming.

Parallel programmingì—ì„œ comminication and synchronization overheadë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.

Appendix B describes an increasingly popular hardware component that is included with desktop computers, the graphics processing unit (GPU). Invented to accelerate graphics, GPUs are becoming programming platforms in their own right. As you might expect, given these times, GPUs rely on parallelism.

### 11. Fallacies and Pitfalls

> Science must begin with myths, and the criticism of myths. _Sir Karl Popper, The Philosophy of Science, 1957_

fallacies: commonly held misconceptions, pitfalls: easily made mistakes. ë­ê°€ ë‹¤ë¥¸ê±°ì§€,,

**Pitfall: Expecting the improvement of one aspect of a computer to increase overall performance by an amount proportional to the size of the improvement.**

> **Amdahl's Law**: Performance enhancement possible with a given improvement is limited by the amount that the improved feature is used.

ìˆ˜í™• ì²´ê°ì˜ ë²•ì¹™?ì˜ quantatitive ë²„ì „ì´ë‹¤.

Execution time after improvement = Execution time affected by improvement / Amount of improvement + Execution time unaffected

**Fallacy: Computers at low utilization use little power.**

êµ¬ê¸€ warehouse scale computerë„ ëŒ€ë¶€ë¶„ì˜ ì‹œê°„ì„ 10%-50%ì—ì„œ ë³´ë‚¸ë‹¤. ë¡œë“œê°€ 10%ì—¬ë„ peak powerì˜ 33%ë‚˜ ì“´ë‹¤. low utilizationì´ë¼ê³  little powerëŠ” ì•„ë‹Œë“¯?

**Fallacy: Designing for performance and designing for energy efficiency are unrelated goals.**

ìµœì í™” ê³¼ì •ì—ì„œ ì—ë„ˆì§€ë¥¼ ì¡°ê¸ˆ ë” ë¨¹ë”ë¼ë„ ì‹¤í–‰ ì‹œê°„ì´ ì¤„ì–´ë“¤ë©´ ì†Œëª¨ ì—ë„ˆì§€ë„ ì¤„ ìˆ˜ ìˆë‹¤.

**Pitfall: Using a subset of the performance equation as a performance metric.**

MIPSëŠ” capabilities of the instructionsì„ ë°˜ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤. ë˜í•œ ê°™ì€ ì»´í“¨í„°ë”ë¼ë„ í”„ë¡œê·¸ë¨ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤. ë˜í•œ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì´ ë” ë§ì€ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ë„ ê° ëª…ë ¹ì–´ê°€ ë” ë¹ ë¥´ë©´ MIPSëŠ” ì„±ëŠ¥ê³¼ ë¬´ê´€í•˜ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.

MIPS = Instruction count / Execution time / 10^6

= Clock rate / CPI / 10^6

**Check Yourself**: A - 4\*10^9 / 10^6 = 4000, B - 3636. A - 2500, B - 2200?

### 12. Concluding Remarks

Perhaps the most important example of abstraction is the interface between hardware and low-level software, called the instruction set architecture.

ISAë¥¼ ìœ ì§€ì‹œì¼œ ê°™ì€ ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ì—¬ëŸ¬ ê³³ì—ì„œ ëŒë¦´ ìˆ˜ ìˆì§€ë§Œ ìœ ì—°í•˜ê²Œ í˜ì‹ ì„ ë„ì…í•˜ê¸° í˜ë“¤ë‹¤.

**The BIG Picture**: Execution time is the only valid and unimpeachable measure of performance. Many other metrics have been proposed and found wanting. Sometimes these metrics are flawed from the start by not reflecting execution time; other times a metric that is sound in a limited context is extended and used beyond that context or without the additional clarification needed to make it valid.

Energy efficiency has replaced die area as the most critical resource of microprocessor design.

## 2. Instructions: Language of the Computer

1-9

### 1. Introduction

> **instruction set**: The vocabulary of commands understood by a given architecture.

Instruction setì€ í•˜ë“œì›¨ì–´ë¥¼ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ë™ì‹œì— ì„±ëŠ¥, ê°€ê²©, ì—ë„ˆì§€ë¥¼ ìµœì í™”í•´ì•¼í•œë‹¤.

> It is easy to see by formal-logical methods that there exist certain [instruction sets] that are in abstract adequate to control and cause the execution of any sequence of operations.... The really decisive considerations from the present point of view, in selecting an [instruction set], are more of a practical nature: simplicity of the equipment demanded by the [instruction set], and the clarity of its application to the actually important problems together with the speed of its handling of those problems.
>
> Burks, Goldstine, and von Neumann, 1946

> **stored-program concept**: The idea that instruction and data of many types can be stored in memory as **numbers** and thus be **easy to change**, leading to the stored-program computer.

### 2. Operations of the Computer Hardware

> There must certainly be instructions for performing the fundamental arithmetic operations.
>
> Burks, Goldstine, and von Neumann, 1946

RISC-Vì˜ ì‚°ìˆ  ëª…ë ¹ì–´ëŠ” í•˜ë‚˜ì˜ ì—°ì‚°ë§Œ í•˜ë©° ì •í™•íˆ ì„¸ ê°œì˜ ë³€ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆì–´ì•¼í•œë‹¤.

í•œ ì¤„ì— ëª…ë ¹ì–´ê°€ ìµœëŒ€ í•œê°œì”©ì´ê³  ì£¼ì„ì€ í•­ìƒ ì¤„ì˜ ëì—ì„œ ëë‚œë‹¤.

RISC-Vì˜ operandì—ëŠ” 32 registerì™€ 2^30 memory wordsê°€ ìˆë‹¤. Byte addressingì„ ì“°ê¸°ì— ì¸ì ‘í•œ wordëŠ” 4ì”© ì°¨ì´ë‚œë‹¤.

load word, halfword, byteê°€ ìˆë‹¤. ê°ê° unsigned ë²„ì „ë„ ìˆë‹¤. storeëŠ” unsigned ë²„ì „ ì—†ë‹¤.

> **What is lr.d and sc.d in RISC-V?**
>
> In RISC-V, "lr.d" and "sc.d" are instructions used for performing atomic memory operations on double-precision floating-point values.
>
> "lr.d" stands for "load reserved double-precision", and it is used to load a double-precision floating-point value from memory while reserving the memory location for exclusive use by the executing processor. This instruction ensures that no other processor can access the memory location until the executing processor completes its atomic operation.
>
> "sc.d" stands for "store conditional double-precision", and it is used to store a double-precision floating-point value to memory only if the memory location has not been modified by another processor since it was reserved by the executing processor using "lr.d". If the memory location has been modified by another processor, the "sc.d" instruction fails and the value is not stored.
>
> Together, "lr.d" and "sc.d" provide a way to perform atomic read-modify-write operations on double-precision floating-point values in shared memory, which is important for synchronization between multiple processors in a multi-core system.

luiëŠ” 20ë¹„íŠ¸.

Conditional branchì™€ jal ëª¨ë‘ PC-relative. **í˜„ì¬ PCê°’**ì— ë”í•œë‹¤. jalrì€ indirect call, PCê°’ì´ ë ˆì§€ìŠ¤í„°ê°’ìœ¼ë¡œ ì¹˜í™˜ëœë‹¤.

To increase portability, Java was originally envisioned as relying on a software interpreter. The instruction set of this interpreter is called **Java bytecodes** (see Section 2.15), which is quite different from the RISC-V instruction set. To get performance close to the equivalent C program, Java systems today typically compile Java bytecodes into the native instruction sets like RISC-V. Because this compilation is normally done much later than for C programs, such Java compilers are often called **Just In Time (JIT) compilers**. Section 2.12 shows how JITs are used later than C compilers in the start-up process, and Section 2.13 shows the performance consequences of compiling versus interpreting Java programs. ë‚˜ì¤‘ì— ì½ì–´ë³´ì,,,

**Check Yourself**: RISC-V > C > Java

### 3. Operands of the Computer Hardware

ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì™€ ë‹¤ë¥´ê²Œ ì‚°ìˆ  ì—°ì‚°ì˜ í”¼ì—°ì‚°ìëŠ” ë°˜ë“œì‹œ ë ˆì§€ìŠ¤í„°ì—¬ì•¼í•œë‹¤.

RISC-Vì—ì„œ ë ˆì§€ìŠ¤í„°ì˜ í¬ê¸°ëŠ” 32ë¹„íŠ¸ì´ë‹¤.

> Does RISC-V support memory bigger than 2^32?
>
> Yes, RISC-V supports memory larger than 2^32 (4 GB). The RISC-V instruction set architecture is designed to be scalable and extensible, which means it can be implemented in systems with a wide range of memory sizes.
>
> The RISC-V specification defines a 64-bit instruction set architecture (ISA), called RV64, which supports 64-bit virtual addresses and can address up to 2^64 bytes of memory (16 exabytes). The RV64 ISA is intended for use in systems with large amounts of memory, such as servers, supercomputers, and embedded systems with high-performance requirements.
>
> In addition to RV64, RISC-V also defines 32-bit and 128-bit ISAs, called RV32 and RV128, respectively. The RV32 ISA supports 32-bit virtual addresses and can address up to 2^32 bytes of memory, while the RV128 ISA supports 128-bit virtual addresses and can address up to 2^128 bytes of memory.
>
> It's worth noting that the size of physical memory that can be addressed by a RISC-V processor is determined by the implementation of the processor, rather than the RISC-V ISA itself. The maximum memory size that can be accessed by a particular RISC-V implementation depends on the number of address lines and memory controllers in the implementation.

> Then what is the register size of RV64?
>
> The register size of RV64 is 64 bits. In the RISC-V ISA, registers are the fundamental units of data storage, and they are used to hold operands, intermediate results, and addresses for memory operations.
>
> The RV64 ISA defines 32 integer registers, each of which is 64 bits wide. These registers are identified by integer numbers ranging from 0 to 31. In addition to integer registers, RV64 also defines 32 floating-point registers, each of which is also 64 bits wide. These floating-point registers are identified by names starting with "f" followed by a number from 0 to 31.
>
> The 64-bit register size of RV64 provides several advantages over 32-bit architectures, such as larger address space, improved performance for applications that require 64-bit precision, and better support for high-performance computing applications that require large amounts of memory.

32ë¹„íŠ¸ê°€ ì›Œë‚™ ìì£¼ ë‚˜ì™€ wordë¼ê³  í•œë‹¤. 64ë¹„íŠ¸ëŠ” doublewordë¼ê³  í•œë‹¤.

> **word**: A natural unit of access in a computer, usually a group of 32bits; corresponds to the size of a register in the RISC-V architecture.

> **data transfer instruction**: A command that moves data between memory and registers.

```txt
lw x9, 8(x22) // Temporary reg x9 gets A[8]

```

The register added to form the address (x22) is called the **base register**, ane the constant in a data transfer instruction (8) is called the **offset**.

RISC-VëŠ” little endian.

> **alignment restriction**: A requirement that data be aligned in memory on natural boundaries. RISC-V/x86ì—ëŠ” ì—†ê³  MIPSì—ëŠ” ìˆë‹¤.

As the addresses in loads and stores are binary numbers, we can see why the DRAM for main memory comes in binary sizes rather than in decimal sizes. That is, in gibibytes (2^30) or tebibytes (2^40), not in gigabytes (10^9) or terabytes (10^12).

Computers divide into those that use the address of the leftmost or â€œbig endâ€ byte as the word address versus those that use the rightmost or â€œlittle endâ€ byte. í‘œí˜„ì´ ë­”ê°€ í—·ê°ˆë¦¼. ì£¼ì†ŒëŠ” ê·¸ëŒ€ë¡œì¸ ê²ƒ ê°™ë‹¤.

> The process of putting less frequently used variables (or those needed later) into memory is called **spilling registers**.

A RISC-V data transfer instruction only reads one operand or writes one operand, without operating on it.

In fact, more than half of the RISC-V arithmetic instructions have a constant as an operand when running the SPEC CPU2006 benchmarks. Constant operands occur frequently; indeed, **addi** is the most popular instruction in most RISC-V programs.

The constant zero has another role, which is to simplify the instruction set by offering useful variations. For example, you can negate the value in a register by using the sub instruction with zero for the first operand. Hence, RISC-V dedicates register x0 to be hard-wired to the value zero. Using frequency to justify the inclusions of constants is another example of the great idea from Chapter 1 of **making the common case fast**.

Although the RISC-V registers in this book are 32 bits wide, the RISC-V architects conceived multiple variants of the ISA. In addition to this variant, known as RV32, a variant named RV64 has 64-bit registers, whose larger addresses make RV64 better suited to processors for servers and smart phones.

The register in the data transfer instructions was originally invented to hold an index of an array with the offset used for the starting address of an array. Thus, the base register is also called the **index register**. Todayâ€™s memories are much larger, and the software model of data allocation is more sophisticated, so the base address of the array is normally passed in a register since it wonâ€™t fit in the offset, as we shall see.

```c
long foo() {
    return 0x1234567812345678;
}
```

```s
.file	"longli.c"
	.option nopic
	.attribute arch, "rv64i2p0_m2p0_a2p0_f2p0_d2p0"
	.attribute unaligned_access, 0
	.attribute stack_align, 16
	.text
	.align	2
	.globl	foo
	.type	foo, @function
foo:
	lui	a5,%hi(.LC0)
	ld	a0,%lo(.LC0)(a5)
	ret
	.size	foo, .-foo
	.section	.srodata.cst8,"aM",@progbits,8
	.align	3
.LC0:
	.dword	1311768465173141112
	.ident	"GCC: (g2ee5e430018-dirty) 12.2.0"
```

The migration from 32-bit address computers to 64-bit address computers left compiler writers a choice of the size of data types in C. Clearly, pointers should be 64 bits, but what about integers? Moreover, C has the data types int, long int, and long long int. The problems come from converting from one data type to another and having an unexpected overflow in C code that is not fully standard compliant, which unfortunately is not rare code. The table below shows the two popular options:

windows - pointer 64, int 32, long int 32, long long int 64

linux, most unix - pointer 64, int 32, long int 64, long long int 64

**Check Yourself**: 2

### 4. Signed and Unsigned Numbers

Since words are drawn vertically as well as horizontally, leftmost and rightmost may be unclear. Hence, the phrase least significant bit is used to refer to the rightmost bit and most significant bit to the leftmost bit.

> **overflow**: when the result of an operation are larger than be represented in a register.

sign and magnitude ë°©ì‹ì€ sign bitì˜ ìœ„ì¹˜ê°€ ì• ë§¤í•˜ê³ , adderì—ì„œ ì¶”ê°€ì ì¸ ë‹¨ê³„ê°€ í•„ìš”í•˜ë©°, +0ê³¼ -0ì´ ëª¨ë‘ ì¡´ì¬í•œë‹¤.

Twoâ€™s complement does have one negative number that has no corresponding positive number: -2,147,483,648ten. ëª¨ë“  ìŒìˆ˜ì˜ MSB(sign bit)ê°€ 1ì´ë¼ëŠ” ì¥ì ì´ ìˆë‹¤.

Overflow occurs when the leftmost retained bit of the binary bit pattern is not the same as the infinite number of digits to the left (the sign bit is incorrect).

The function of a signed load is to copy the sign repeatedly to fill the rest of the registerâ€”called sign extensionâ€”but its purpose is to place a correct representation of the number within that register. Unsigned loads simply fill with 0s to the left of the data, since the number represented by the bit pattern is unsigned. Wordë¥¼ loadí•  ë•ŒëŠ” ì˜ë¯¸ ì—†ë‹¤. Cì—ì„œëŠ” ë°”ì´íŠ¸ë¥¼ very short signed integerë³´ë‹¤ëŠ” charì—ì„œë‚˜ ì“°ë‹ˆ ëŒ€ë¶€ë¶„ lbê°€ ì•„ë‹Œ lbuê°€ ì‚¬ìš©ëœë‹¤.

Twoâ€™s complement gets its name from the rule that the unsigned sum of an n-bit number and its n-bit negative is 2^n; hence, the negation or complement of a number x is 2n âˆ’ x, or its â€œtwoâ€™s complement.â€

one's complementëŠ” ë¹„íŠ¸ ë°˜ì „ë§Œ ì‹œì¼œì„œ ìŒìˆ˜ë¥¼ ì–»ëŠ”ë‹¤. 0ì´ ë‘ê°œê³  adderì—ì„œ 1ì„ ë¹¼ì£¼ëŠ” ì¶”ê°€ì ì¸ ìŠ¤í…ì´ í•„ìš”í•˜ë‹¤. ë¶€ë™ ì†Œìˆ˜ì ì—ì„œ ì–¸ê¸‰ë  biased notationë„ ìˆë‹¤. biasë¥¼ ë”í•˜ë©´ non-negative notationì´ ëœë‹¤.

**Check Yourself**: 2, 4

### 5. Representing Instructions in the Computer

> **instruction format**: A form of representation of an instruction composed of fields of binary numbers.

> **machine language**: Binary representation used for communication within a computer system.

RISC-Vì˜ í•„ë“œëŠ” ì•„ë˜ì™€ ê°™ì´ êµ¬ì„±ëœë‹¤.

R-type: funct7(7bits), rs2(5bits), rs1(5bits), funct3(3bits), rd(5bits), opcode(7bits)

> **opcode**: The field that denotes the **operation** and **format** of and instruction.

I-type: immediate(12bits), rs1(5bits), funct3(3bits), rd(5bits), opcode(7bits)

S-type: immdiate\[11:5](7bits), rs2(5bits), rs1(5bits), funct3(3bits), immediate\[4:0](5bits), opcode(7bits)

```txt
sw x9, 120(x10)
-> rs2ê°€ 9, rs1ì´ 10
```

ì´ëŸ° ê¸°ë¬˜í•œ í˜•íƒœë•ë¶„ì— rs1ê³¼ rs2ê°€ í•­ìƒ ê°™ì€ ìœ„ì¹˜ì— ìˆì„ ìˆ˜ ìˆë‹¤. opcodeì™€ func3ì€ ê°™ì€ í¬ê¸° ê°™ì€ ìœ„ì¹˜ì— ìˆë‹¤. **í¬ë§·ì˜ ì¢…ë¥˜ëŠ” opcode fieldë¥¼ í†µí•´ ê²°ì •ëœë‹¤**.

RISC-V assembly language programmers arenâ€™t forced to use addi when working with constants. The programmer simply writes add, and the assembler generates the proper opcode and the proper instruction format depending on whether the operands are all registers (R-type) or if one is a constant (I-type).

addì™€ subëŠ” ìˆì§€ë§Œ subiëŠ” ì—†ë‹¤.

**The BIG Picture**: Instructions are represented as numbers + Programs are stored in memory to be read or written, just like data -> stored-prograrm concept. One consequence of instructions as numbers is that programs are often shipped as files of binary numbers. The commercial implication is that computers can inherit ready-made software provided they are compatible with an existing instruction set. Such â€œbinary compatibilityâ€ often leads industry to align around a small number of instruction set architectures.

The memory technology needed for data can also be used for programs, and programs like compilers, for instance, can translate code written in a notation far more convenient for humans into code that the computer can understand.

**Check Yourself**: **3**, 0x32

### 6 Logical Operations

Logical operations were added to programming languages and instruction set architectures to simplify, among other things, the **packing and unpacking of bits into words**.

slli, srli. lì´ ë¬´ì¡°ê±´ ë¶™ë„¤? srai.

ì‹œí”„íŠ¸ ëª…ë ¹ì–´ëŠ” I-type formatì„ ì“°ì§€ë§Œ 31ì„ ë„˜ì–´ ì‹œí”„íŠ¸í•˜ëŠ” ê²ƒì€ ë¬´ì˜ë¯¸í•˜ë¯€ë¡œ 5ë¹„íŠ¸ë§Œ ì“°ê³  ë‚˜ë¨¸ì§€ëŠ” **funct7ìœ¼ë¡œ ì¬ì‚¬ìš©**ëœë‹¤.

func7(7bits) + immediate(4bits) + rs1(5bits) + func3(3bits) + rd(5bit) + opcode(7bits)

As you can see, AND can apply a bit pattern to a set of bits to force 0s where there is a 0 in the bit pattern. Such a bit pattern in conjunction with AND is traditionally called a **mask**, since the mask â€œconcealsâ€ some bits.

In keeping with the three-operand format, the designers of RISC-V decided to include the instruction XOR (exclusive OR) instead of NOT. Since exclusive OR creates a 0 when bits are the same and a 1 if they are different, the equivalent to NOT is an xor 111...111.

C allows **bit fields** or **fields** to be defined within words, both allowing objects to be packed within a word and to match an externally enforced interface such as an I/O device. All fields must fit within a single word. Fields are unsigned integers that can be as short as 1 bit. C compilers insert and extract fields using logical instructions in RISC-V: andi, ori, slli, and srli.

```c
#include <stdlib.h>

typedef struct {
    unsigned int is_active : 1;
    unsigned int has_permission : 1;
    unsigned int is_admin : 1;
} flags;

void foo(flags* user_flags, int is_active, int has_permission, int is_admin) {
    user_flags->is_active = is_active;
    user_flags->has_permission = has_permission;
    user_flags->is_admin = is_admin;
}
```

```s
.file	"bitfield.c"
	.option nopic
	.attribute arch, "rv64i2p0_m2p0_a2p0_f2p0_d2p0"
	.attribute unaligned_access, 0
	.attribute stack_align, 16
	.text
	.align	2
	.globl	foo
	.type	foo, @function
foo:
	lbu	a5,0(a0)
	andi	a2,a2,1
	slliw	a2,a2,1
	andi	a1,a1,1
	andi	a3,a3,1
	or	a1,a1,a2
	slliw	a3,a3,2
	or	a1,a1,a3
	andi	a5,a5,-8
	or	a1,a1,a5
	sb	a1,0(a0)
	ret
	.size	foo, .-foo
	.ident	"GCC: (g2ee5e430018-dirty) 12.2.0"
```

lbuì™€ sbê°€ í¬ì¸íŠ¸ì¸ë“¯.

[In case of bit fields, which one is better to use, unsigned char or unsigned int and why?](https://stackoverflow.com/questions/17670436/in-case-of-bit-fields-which-one-is-better-to-use-unsigned-char-or-unsigned-int)

**Check Yourself**: Both. Note that AND leaves the field where it was originally, and the shift pair moves the field into the rightmost part of the doubleword.

(The expression "A followed by B" is usually understood to mean that A happens first, followed by B. )

### 7. Instructions for Making Decisions

> **conditional branch**: An instruction that tests a value and that allows for a subsequent transfer of control to a new address in the program based on the outcome of the test.

In general, the code will be more efficient if we test for the opposite condition to branch over the code that branches if the values are not equal (bne)???

(See Self Study in Section 2.25 for an optimization of this sequence.) -> ë‚˜ì¤‘ì— ì‚´í´ë³´ê¸°

Such sequences of instructions that end in a branch are so fundamental to compiling that they are given their own buzzword: a **basic block** is a sequence of instructions without branches, except possibly at the end, and without branch targets or branch labels, except possibly at the beginning. One of the first early phases of compilation is breaking the program into basic blocks.

beq, bne, blt, bge, bltu, bgeu.

Yet another alternative, used by ARMâ€™s instruction sets, is to keep extra bits that record what occurred during an instruction. These additional bits, called **condition codes** or **flags**, indicate, for example, if the result of an arithmetic operation was negative, or zero, or resulted in overflow.

```txt
// Treating signed numbers as if they were unsigned gives us a low-cost way of checking if 0 â‰¤ x < y, which matches the index out-of-bounds check for arrays.

bgeu x20, x11, IndexOutOfBounds
// if x20 >= x11 or x20 < 0, goto IndexOutOfBounds
```

> **branch address table**: Also called branch table. A table of addresses of alternative instruction sequences.

Branch tableì„ í†µí•´ switchë¬¸ì„ íš¨ê³¼ì ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ ìœ„í•´ RISC-Vì—ì„œëŠ” jalrê°™ì€ indirect jump ëª…ë ¹ì–´ë¥¼ ì§€ì›í•œë‹¤. GPTí•œí…Œ ë¬¼ì–´ë³´ëŠ” ì¤‘ì¸ë° sltë¼ëŠ” ëª…ë ¹ì–´ë„ ìˆë‚˜?

```txt
switch:
    addi sp, sp, -4         # decrement stack pointer
    sw   ra, 0(sp)          # save return address on stack
    li   t1, 4              # load 4 into temporary register t1
    mul  s0, s0, t1         # multiply input value by 4 to get offset into table
    add  t0, s0, x10        # add offset to address of table (assuming table is at address x10)
    lw   t0, 0(t0)          # load address of target label from table into t0
    jalr ra, t0, 0          # jump to target label

    .align 2                # align table on 4-byte boundary
table:
    .word case_1            # entry 0: jump to case_1
    .word case_2            # entry 1: jump to case_2
    .word case_3            # entry 2: jump to case_3
    .word default           # entry 3: jump to default

case_1:
    # code for case 1
    j end_switch            # jump to end of switch statement

case_2:
    # code for case 2
    j end_switch            # jump to end of switch statement

case_3:
    # code for case 3
    j end_switch            # jump to end of switch statement

default:
    # code for default case
    j end_switch            # jump to end of switch statement

end_switch:
    lw ra, 0(sp)            # restore return address from stack
    addi sp, sp, 4          # increment stack pointer
    jr  ra
```

**Check Yourself**: I-1,2,3. II-3?. ë­ì•¼ ë‹¤í‹€ë¦¼,,

```c
int foo(int a, int b) {
    if (a && b)
        return 0;
    else
        return 1;
}
```

```s
.file	"andImpl.c"
	.option nopic
	.attribute arch, "rv64i2p0_m2p0_a2p0_f2p0_d2p0"
	.attribute unaligned_access, 0
	.attribute stack_align, 16
	.text
	.align	2
	.globl	foo
	.type	foo, @function
foo:
	beq	a0,zero,.L3
	seqz	a0,a1
	ret
.L3:
	li	a0,1
	ret
	.size	foo, .-foo
	.ident	"GCC: (g2ee5e430018-dirty) 12.2.0"
```

### 8. Supporting Procedures in Computer Hardware

> **procedure**: A stored subroutine that performs a specific task based on the parameters with which it is provided.

í•¨ìˆ˜ íŒ¨ëŸ¬ë¯¸í„°ëŠ” í”„ë¡œì‹œì €ì™€ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ ë° ë°ì´í„° ì‚¬ì´ì˜ ì¸í„°í˜ì´ìŠ¤ ì—­í• ì„ í•œë‹¤. í”„ë¡œì‹œì €ëŠ” ì¶”ìƒí™”ë¥¼ êµ¬í˜„í•˜ëŠ” í•œ ë°©ë²•ì´ë‹¤.

x10-x17ì—ëŠ” íŒ¨ëŸ¬ë¯¸í„°, x1ì—ëŠ” ë¦¬í„´ ì£¼ì†Œë¥¼ ë†“ëŠ”ë‹¤.

> **jump-and-link instruction**: An instruction that branches to an address and simultaneously saves the address of the following instruction in register.

> **return address**: A link to the calling site that allows a procedure to return to the proper address.

> **program counter(PC)**: The register containing the address of the instruction in the program being executed.

> **stack**: A data structure for spilling registers organized as a last-in-first-out queue.

jal, jalr ëª¨ë‘ unconditional branchì— ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. jalrì€ í•¨ìˆ˜ ë¦¬í„´ì—ì„œ x1ì„ í†µí•´ ë¬´ì¡°ê±´? ì‚¬ìš©ë˜ê³  jalì€ ì–˜ë„ë¡œ ëœë‹¤~ ëŠë‚Œì¸ë“¯.

By historical precedent, stacks â€œgrowâ€ from higher addresses to lower addresses. This convention means that you push values onto the stack by subtracting from the stack pointer. Adding to the stack pointer shrinks the stack, thereby popping values off the stack.

í•œë²ˆë„ ì‚¬ìš©ë˜ì§€ ì•Šì€ ë ˆì§€ìŠ¤í„°ë¥¼ ë°±ì—…/ë³µì›í•˜ëŠ” ì°¸ì‚¬ë¥¼ ë§‰ê¸° ìœ„í•´ RISC-Vì—ì„œëŠ” 19ê°œì˜ ë ˆì§€ìŠ¤í„°ë¥¼ ë‘ ë¶€ë¥˜ë¡œ ë‚˜ëˆˆë‹¤.

- x5âˆ’x7 and x28âˆ’x31: temporary registers that are not preserved by the callee (called procedure) on a procedure call
- x8âˆ’x9 and x18âˆ’x27: saved registers that must be preserved on a procedure call (if used, the callee saves and restores them)

Procedures that do not call others are called leaf procedures.

```c
#include <stdio.h>

int main() {
    int a;
    printf("%p\n", &a);
}
```

```s
.file	"address.c"
	.text
	.section	.rodata.str1.8,"aMS",@progbits,1
	.align	3
.LC0:
	.string	"%p\n"
	.text
	.align	2
	.globl	main
main:
	addi	sp,sp,-32
	sd	ra,24(sp)           # ra ì €ì¥ (+24~+32)
	addi	a1,sp,12        # a1 = sp+12??
	lui	a0,%hi(.LC0)
	addi	a0,a0,%lo(.LC0) # MSBê°€ 1ì´ë©´ ì–´ë–¡í•¨??
	call	printf
	li	a0,0
	ld	ra,24(sp)
	addi	sp,sp,32
	jr	ra
	.size	main, .-main
```

```c
int foo() {
    return 0xdeadbeef;
}
```

```s
.file	"deadbeef.c"
	.text
	.align	2
	.globl	foo
foo:
	li	a0,-559038464  # 0xDEADC000
	addi	a0,a0,-273 # 0xFEEF
	ret
	.size	foo, .-foo
```

A C variable is generally a location in storage, and its interpretation depends both on its type and storage class. Example types include integers and characters (see Section 2.9). C has two storage classes: automatic and static. Automatic variables are local to a procedure and are discarded when the procedure exits. Static variables exist across exits from and entries to procedures. C variables declared outside all procedures are considered static, as are any variables declared using the keyword static. The rest are automatic. To simplify access to static data, some RISC-V compilers reserve a register x3 for use as the global pointer, or gp.

> **global pointer**: The register that is reserve dto point to the static area.

- Preserved: Saved registers(x8-9, x18-27), Stack pointer register(x2, sp), Frame pointer(x8, fp), Return address(x1, ra), Stack above the stack pointer
- Not preserved: Temporary registers(x5-7, x28-31), Argument/result registers(x10-17), Stack below the stack pointer.

Stack above/below ì˜ë¯¸ ì•Œê¸°!

> **procedure frame(activation record)**: The segment of the stack containing a procedure's saved registers and local variables.

> **frame pointer**: A value denoting the location of the saved registers and local variables for a given procedure.

Frame pointer offers a stable base register within a procedure for local memory-references.

When a frame pointer is used, it is initialized using the address in sp on a call, and sp is restored using fp.

> **text segment**: The segment of a UNIX object file that contains the machine language code for routines in the source file.

ë‚®ì€ ì£¼ì†Œê°’ë¶€í„° reserved -> text(0x40_0000) -> static data(0x100_00000) -> dynamic data <- stack(~0x3f_ffff_fff0). ì£¼ì†Œê°’ì€ software conventionìœ¼ë¡œ RISC-V architectureì˜ ì¼ë¶€ê°€ ì•„ë‹ˆë‹¤.

This convention is another example of making the common case fast: most procedures can be satisfied with up to eight argument registers, twelve saved registers, and seven temporary registers without ever going to memory.

What if there are more than eight parameters? The RISC-V convention is to place the extra parameters on the stack just above the frame pointer.

The RISC-V C compiler only uses a frame pointer in procedures that change the stack pointer in the body of the procedure.

```c
int sum (int n, int acc) {
    if (n > 0)
        return sum(n âˆ’ 1, acc + n);
    else
        return acc;
}
```

tail call optimizationì´ ê°€ëŠ¥í•˜ë‹¤. -O3ì„ ì¤˜ì•¼ ì˜í•´ì£¼ë„¤

```s
	.file	"tailcall.c"
	.text
	.align	2
	.globl	sum
sum:
	mv	a5,a0
	mv	a0,a1
	ble	a5,zero,.L2
.L3:
	mv	a4,a5
	addiw	a5,a5,-1
	addw	a0,a4,a0
	bne	a5,zero,.L3
.L2:
	ret
	.size	sum, .-sum
```

**Check Yourself**: both

### 9. Communicating with People

ASCIIì—ì„œ ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìëŠ” 32 ì°¨ì´ë‚œë‹¤.

Because of the popularity of text in some programs, however, RISC-V provides instructions to move bytes. Load byte unsigned (lbu) loads a byte from memory, placing it in the rightmost 8 bits of a register. Store byte (sb) takes a byte from the rightmost 8 bits of a register and writes it to memory.

```c
void foo(char arr[]) {
    arr[0] = 'a';
    arr[1] = 'b';
}
```

```s
.file	"sb.c"
	.text
	.align	2
	.globl	foo
foo:
	li	a5,97
	sb	a5,0(a0)
	li	a5,98
	sb	a5,1(a0)
	ret
	.size	foo, .-foo
```

When a compiler finds a leaf procedure, it exhausts all temporary registers before using registers it must save.

ê¸°ë³¸ì ìœ¼ë¡œ ìœ ë‹ˆì½”ë“œëŠ” characterë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ 16ë¹„íŠ¸ë¥¼ ì‚¬ìš©í•œë‹¤.

A 16-bit encoding, called UTF-16, is the default. A variable-length encoding, called UTF-8, keeps the ASCII subset as eight bits and uses 16 or 32 bits for the other characters. UTF-32 uses 32 bits per character.

```c
#include <stdio.h>

int main() {
    printf("ğŸ˜€\n");
    printf("\360\237\230\200\n");
}
```

```s
.file	"emoji.c"
	.text
	.section	.rodata.str1.8,"aMS",@progbits,1
	.align	3
.LC0:
	.string	"\360\237\230\200"
	.section	.text.startup,"ax",@progbits
	.align	2
	.globl	main
main:
	addi	sp,sp,-16
	sd	s0,0(sp)
	lui	s0,%hi(.LC0)
	addi	a0,s0,%lo(.LC0)
	sd	ra,8(sp)
	call	puts
	addi	a0,s0,%lo(.LC0)
	call	puts
	ld	ra,8(sp)
	ld	s0,0(sp)
	li	a0,0
	addi	sp,sp,16
	jr	ra
	.size	main, .-main
```

Newcomers to computing are surprised that the type of the data is not encoded inside the data but instead in the program that operates on that data. ... This unrestricted behavior is why file systems have a naming convention of the suffix giving the type of file (e.g., .jpg, .pdf, or .txt) to enable the program to check for mismatches by file name to reduce the occurrence of such embarrassing scenarios.

**Check Yourself**: I-1,**2**. II-3

### 10. RISC-V Addressing for Wide Immediates and Addresses

ë²”ìœ„ëŠ” ì•„ë‹Œë° ë‚´ìš©ì´ ë„ì›€ì€ ë  ë“¯?

## 3. Arithmetic for Computers

1-5

### 1. Introduction

Representation of real numbers, arithmetic algorithms, hardware that follows these algorithmsâ€” and the implications of all this for instruction sets.

### 2. Addition and Subtraction

ë‹¤ë¥¸ ë¶€í˜¸ê°„ ë§ì…ˆì—ì„œëŠ” ì˜¤ë²„í”Œë¡œìš°ê°€ ë°œìƒí•  ìˆ˜ ì—†ë‹¤. ëº„ì…ˆì—ì„œëŠ” ë¶€í˜¸ê°€ ê°™ìœ¼ë©´ ë°œìƒí•  ìˆ˜ ì—†ë‹¤.

ê²°ê³¼ê°€ í”¼ì—°ì‚°ìì™€ ë¶€í˜¸ê°€ ë‹¤ë¥´ë©´ ì˜¤ë²„í”Œë¡œìš°ì´ë‹¤. Carry out occurred into the sign bit / Borrow occurred from the sign bit.

Fortunately, the compiler can easily check for unsigned overflow using a branch instruction. Addition has overflowed if the sum is less than either of the addends, whereas subtraction has overflowed if the difference is greater than the minuend.

Itâ€™s easy to detect overflow in unsigned numbers, although these are almost always ignored because programs donâ€™t want to detect overflow for address arithmetic, the most common use of natural numbers.

> **Arithmetic Logic Unit(ALU)**: Hardware that performs addition, subtraction, and usually logical operations such as AND or OR.

Some programming languages allow twoâ€™s complement integer arithmetic on variables declared byte and half, whereas **RISC-V only has integer arithmetic operations on full words.**

One feature not generally found in general-purpose microprocessors is **saturating operations**. Saturation means that when a calculation overflows, the result is set to the largest positive number or the most negative number, rather than a modulo calculation as in twoâ€™s complement arithmetic. (e.g. media operations)

**Check Yourself**: RISC-V only has integer arithmetic operations on fuil words. 2

### 3 Multiplication

The first operand is called the **multiplicand** and the second the **multiplier**. The final result is called the product.

**n + m bits** are required to represent all possible products.

Traditional methodì—ì„œëŠ” multiplicandë¥¼ 32ë²ˆ shift rightí•˜ë‹ˆ ì´ 64ë¹„íŠ¸ multiplicand registerê°€ í•„ìš”í•˜ë‹¤. 1. Multiplierì˜ LSBë¥¼ í™•ì¸í•˜ê³  1ì´ë©´ product register ì—…ë°ì´íŠ¸, 2. multiplicand shift left, 3. multiplier shift right. ì´ëŒ€ë¡œë©´ 3ì‚¬ì´í´ì´ì§€ë§Œ ë§ì…ˆì„ í•˜ëŠ”ë™ì•ˆ shiftí•˜ë©´ 1ì‚¬ì´í´ì— ê°€ëŠ¥í•˜ë‹¤. The hardware only has to ensure that it tests the right bit of the multiplier and gets the **preshifted** version of the multiplicand.

The relative importance of arithmetic operations like multiply varies with the program, but addition and subtraction may be anywhere from 5 to 100 times more popular than multiply. Accordingly, in many applications, multiply can take several clock cycles without significantly affecting performance. **However, Amdahlâ€™s Law (see Section 1.10) reminds us that even a moderate frequency for a slow operation can limit performance.**

```c
int foo(int x) {
    return x * 96;
}
```

```s
.file	"mulOptimize.c"
	.text
	.align	2
	.globl	foo
foo:
	slliw	a5,a0,1
	addw	a0,a5,a0
	slliw	a0,a0,5
	ret
	.size	foo, .-foo
```

Traditional methodì—ì„œ productì˜ ì˜¤ë¥¸ìª½ ì ˆë°˜ì´ ë¹„ì–´ìˆê³  ë§ˆì¹¨ multiplierë„ í•œì¹¸ì”© ì‚¬ë¼ì§€ë‹ˆ ì–˜ë„¤ ë‘˜ì„ í•©ì³ë³´ì. Productë¥¼ shift rightí•œë‹¤. ì´ì œ product registerëŠ” 32bit adderì˜ carry-outì„ ë°›ê¸° ìœ„í•´ 65ë¹„íŠ¸ì´ë‹¤. ì™¼ìª½ ê³ ì • ìœ„ì¹˜ì— ì¨ì„œ ê·¸ëŸ°ë“¯? ê·¼ë° ê·¸ë¦¼ì€ ì™œì €ëŸ¼.

The easiest way to understand how to deal with signed numbers is to first convert the multiplier and multiplicand to positive numbers and then remember their original signs. ì´í›„ sign bit ë¹¼ê³  31ë‹¨ê³„ë¥¼ ê±°ì¹œë‹¤.

ì‚¬ì‹¤ last algorithmì—ì„œ shifting stepì—ì„œ sign extensionë§Œ í•´ì¤˜ë„ ëœë‹¤? When the algorithm completes, the lower word would have the '32-bit' product??

A straightforward approach would be to connect the outputs of adders on the right to the inputs of adders on the left, making a stack of adders 64 high. An alternative way to organize these 32 additions is in a parallel tree, as Figure 3.7 shows. Instead of waiting for 32 add times, we wait just the log2 (32) or five 32-bit add times.

In fact, multiply can go even faster than six add times because of the use of carry save adders (see Section A.6 in Appendix A), and because it is easy to pipeline such a design to be able to support many multiplies simultaneously (see Chapter 4).

mul, mulh, mulhu, mulhsu. 32bitê³±ì´ë©´ mul, 64ê³±ì˜ ìœ„ìª½ì€ signedë©´ mulh, unsignedë©´ mulhu, ë‹¤ë¥´ë©´ mulhsu. ê·¸ëŸ°ë° í•´ë³´ë‹ˆê¹Œ mulwê°€ ë‚˜ì˜¤ëŠ”ë°?

Software can use the multiply-high instructions to check for **overflow from 32-bit multiplication**. There is no overflow for 32-bit unsigned multiplication if mulhuâ€™s result is zero. There is no overflow for 32-bit signed multiplication if all of the bits in mulhâ€™s result are copies of the sign bit of mulâ€™s result. ì—¬ê¸°ì„œ ì˜¤ë²„í”Œë¡œìš°ë€ 32ë¹„íŠ¸ì— ì•ˆë“¤ì–´ê°€ëŠ” ê°’ì„ ë§í•œë‹¤.

### 4. Division

> **dividend**: A number being divided.

> **divisor**: A number that the dividend is divided by.

```c
int quotient(int dividend, int divisor) {
    return dividend / divisor;
}

int remainder(int dividend, int divisor) {
    return dividend % divisor;
}
```

```s
.file	"div.c"
	.text
	.align	2
	.globl	quotient
quotient:
	divw	a0,a0,a1
	ret
	.size	quotient, .-quotient
	.align	2
	.globl	remainder
remainder:
	remw	a0,a0,a1
	ret
	.size	remainder, .-remainder
```

ì²«ë²„ì „ì—ì„œëŠ” 64bit divisor register, 32bit quotient register, 64bit remainder register, 64bit ALUë¥¼ ì‚¬ìš©í•œë‹¤. Divisorì˜ ìš°ì¸¡ 32bitì— dividorë¥¼ ë„£ëŠ”ë‹¤. Remainder - Divisorì´í›„ ë¶€í˜¸ë¥¼ ë³´ê³  ìŒìˆ˜ë©´ ë‹¤ì‹œ ë”í•´ ì›ìƒë³µêµ¬í•˜ê³ , ì–´ì°Œëë“  quotientë¥¼ shift leftí•˜ê³  0ë˜ëŠ” 1ì„ ìš°ì¸¡ì— ë„£ëŠ”ë‹¤. ì´í›„ divisorë¥¼ shift rightí•œë‹¤. ì´ê±¸ 32ë²ˆ ë°˜ë³µ.

Remainder ìš°ì¸¡ ë¹„ì–´ìˆê³  QuotientëŠ” í•˜ë‚˜ì”© ì¤„ì–´ë“œë‹ˆ ìµœì í™” ê°€ëŠ¥í•˜ë‹¤. Remainder ì˜¤ë¥¸ìª½ì— quotientë¥¼ ë„£ê³  ì™¼ìª½ ì‹œí”„íŠ¸ë¥¼ í•œë‹¤. ê³±ì…ˆê³¼ ë§ˆì°¬ê°€ì§€ë¡œ carry outì´ ì‚¬ë¼ì§€ì§€ ì•Šê²Œ remainer registerê°€ 65bitì´ë‹¤.

So far, we have ignored signed numbers in division. The simplest solution is to remember the signs of the divisor and dividend and then negate the quotient if the signs disagree.

Dividend = Quotient \* Divisor + Remainderê°€ ì„±ë¦½. -7 ë‚˜ëˆ„ê¸° 2ì˜ ëª«ì´ -4ê³  ë‚˜ë¨¸ì§€ê°€ 1ì´ë©´ 7 ë‚˜ëˆ„ê¸° 2ì™€ ëª«ì˜ ì ˆëŒ€ê°’ì´ ë‹¬ë¼ì§„ë‹¤. -(x / y) != (-x) / yë©´ ë¨¸ë¦¬ ê¹¨ì§€ë‹ˆ dividendì™€ remainderê°€ ê°™ì€ ë¶€í˜¸ì´ë„ë¡ í•œë‹¤. Thus, the correctly signed division algorithm negates the quotient if the signs of the operands are opposite and makes the sign of the nonzero remainder match the dividend.

We used many adders to speed up multiply, but we cannot do the same trick for divide. The reason is that we need to know the sign of the difference before we can perform the next step of the algorithm, whereas with multiply we could calculate the 32 partial products immediately.

There are techniques to produce more than one bit of the quotient per step. The **SRT division technique** tries to predict several quotient bits per step, using a table lookup based on the upper bits of the dividend and remainder. ì˜¤ëŠ˜ë‚ ì—ëŠ” 4ë¹„íŠ¸ë¥¼ ì“´ë‹¤.

ê³±ì…ˆê³¼ ë‚˜ëˆ—ì…ˆì— ë™ì¼í•œ í•˜ë“œì›¨ì–´ê°€ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. The only requirement is a 64-bit register that can shift left or right and a 32-bit ALU that adds or subtracts. ê·¸ë˜ì„œ figure 1.1ì— shift rightë„ ìˆë‚˜?

div, divu, rem, remu ê°€ ìˆë‹¤. immediate ë²„ì „ì€ ì—†ë‚˜?

RISC-V divide instructions ignore overflow, so software must determine whether the quotient is too large. ëª«ì´ í´ ìˆ˜ë„ ìˆë‚˜?

An even faster sequential algorithm than Figure 3.9 does not immediately add the divisor back if the remainder is negative. It simply adds the dividend to the shifted remainder in the following step, since (r + d) Ã— 2 âˆ’ d = r âˆ’ 2 + d Ã— 2 âˆ’ d = r Ã— 2 + d. This **nonrestoring division algorithm**, which takes one clock cycle per step, is explored further in the exercises; the algorithm in Figure 3.9 is called **restoring division**. A third algorithm that doesnâ€™t save the result of the subtract if itâ€™s negative is called a **nonperforming division algorithm**. It averages one-third fewer arithmetic operations.

### 5. Floating Point

> **scientific notation**: A notation that renders numbers with a single digit to the left of the decimal point.

> **normalized**: A number in floating-point notation that has no leading 0s.

1.0 \* 10^9ëŠ” normalized numberì´ì§€ë§Œ 0.1 \* 10^-8ì€ ì•„ë‹ˆë‹¤.

> **floating point**: Computer arithmetic that represents numbers in which the binary point is not fixed.

> **fraction**: The value, generally between 0 and 1, placed in the fraction field. The fraction is also called the **mantissa**.

> **exponent**: In the numerical representation system of floating-point arithmetic, the value that is placed in the exponent field.

fractionì€ ì •ë°€ë„ì—, exponentëŠ” ë²”ìœ„ì™€ ê´€ë ¨ë˜ì–´ìˆë‹¤.

> **overflow(floating-point)**: A situation in which a positive exponent becomes too large to fit in the exponent field.

> **underflow(floating-point)**: A situation in which a negative exponent becomes too large to fit in the exponent field.

ì˜¤ë²„í”Œë¡œìš°ëŠ” ê·¸ëŸ¼ fractionì˜ ë¶€ì¡±ì´ë‘ì€ ê´€ê³„ì—†ëŠ”ê±´ê°€?

double precesionì€ 64ë¹„íŠ¸, single precesionì€ 32ë¹„íŠ¸. 1 + 8 + 23, 1 + 11 + 52. Double precesionì˜ ì¥ì ì€ expë³´ë‹¤ëŠ” í° fracì„ í†µí•œ ì •ë°€ì„±ì— ìˆë‹¤.

The address of the instruction that overflowed is saved in a register, and the computer jumps to a predefined address to invoke the appropriate routine for that exception.

RISC-VëŠ” ì˜¤ë²„í”Œë¡œìš°ë‚˜ ì–¸ë”í”Œë¡œìš°ì— exceptionì„ ë°œìƒì‹œí‚¤ì§€ëŠ” ì•Šì§€ë§Œ floating-point control and status register(fcsr)ë¥¼ í™•ì¸í•´ë³¼ ìˆ˜ ìˆë‹¤.

IEEE 754 floating-point standard.

fractionì— ìƒëµëœ 1ì„ ë”í•œ ê²ƒì„ significandë¼ í•˜ì. 0ì´ë©´ ì•ˆë¶™ì´ê³  0ì´ë‹¤.

For the mathematically trained, the purpose of infinity is to form topological closure of the reals.

The purpose of NaNs is to allow programmers to postpone some tests and decisions to a later time in the program when they are convenient.

- expê°€ 0ì¸ë° fracì´ 0ì´ë©´ 0, ì•„ë‹ˆë©´ denormalized.
- expê°€ ìµœëŒ€ê°’ì¸ë° fracì´ 0ì´ë©´ inf, ì•„ë‹ˆë©´ NaN
- ë‚˜ë¨¸ì§€ëŠ” floating-point number

exponentê°€ significantë³´ë‹¤ ì•ì— ìˆìœ¼ë‹ˆ ì •ë ¬ì— ìš©ì´í•˜ë‹¤. ìŒìˆ˜ë©´ ë¶ˆí¸í•˜ë‹ˆ biasë¥¼ ë„£ëŠ”ë‹¤. ê°€ì¥ ì‘ì€ exponentë¥¼ 0ìœ¼ë¡œ, ê°€ì¥ í° exponentë¥¼ 1111...ë¡œ í‘œí˜„í•œë‹¤. **biased notation**. biasëŠ” 128ì´ ì•„ë‹ˆë¼ 127ì´ë‹¤. denormalizedë‘ ê´€ê³„ìˆëŠ”ê±´ê°€?

The revised standard IEEE 754-2008 includes nearly all the IEEE 754-1985 and adds a 16-bit format (â€œhalf precisionâ€) and a 128-bit format (â€œquadruple precisionâ€).

For the example below, remember that for single precision, the maximum exponent is 127, and the minimum exponent is âˆ’126.

1. Compare the exponents of the two numbers; shift the smaller number to the right until its exponent would match the larger exponent
1. Add the significands
1. Normalize the sum, either shifting right and incrementing the exponent or shifting left and decrementing the exponent.
1. If overflow, exception.
1. Round the significand to the appropriate number of bits
1. Back to normalized if not.

ê³±ì…ˆì—ì„œ underflow may occur if both operands are smallâ€”that is, if both have large negative exponents.

Figure 3.15 ë˜ ë³´ê¸°.

ë¹„íŠ¸ ìë¥´ëŠ”ê±°ë¥¼ ê³±ì…ˆ ì§í›„ì—ë„ í•˜ë„¤?

fadd.s, fadd.b, sub, mul, div, sqrt, eq, lt, le...

flw, fld, fsw, fsd.

```c
int foo(float a, float b) {
    return a < b;
}
```

```s
.file	"floatCmp.c"
	.text
	.align	2
	.globl	foo
foo:
	flt.s	a0,fa0,fa1
	ret
	.size	foo, .-foo
```

The RISC-V designers decided to add separate floating-point registers. They are called f0, f1, ..., f31. Hence, they included separate loads and stores for floating-point registers: fld and fsd for double-precision and flw and fsw for single-precision. The base registers for floating-point data transfers which are used for addresses remain integer registers.

```c
void foo(float arr[]) {
    float temp = arr[0];
    arr[0] = arr[1];
    arr[1] = temp;
}
```

```s
.file	"floatLoad.c"
	.text
	.align	2
	.globl	foo
foo:
	flw	fa5,0(a0)
	flw	fa4,4(a0)
	fsw	fa4,0(a0)
	fsw	fa5,4(a0)
	ret
	.size	foo, .-foo
```

f0-f31 ë ˆì§€ìŠ¤í„°ëŠ” float/double ëª¨ë‘ ë‹´ì„ ìˆ˜ ìˆë‹¤. doubleì€ ë‚˜ëˆ ì„œ ë‹´ë‚˜?

The benefits of separate floating-point registers are having twice as many registers without using up more bits in the instruction format, having twice the register bandwidth by having separate integer and floating-point register sets, and being able to customize registers to floating point; for example, some computers convert all sized operands in registers into a single internal format?

Compiling Floating-Point C Procedure with Two-Dimensional Matrices into RISC-V ì˜ˆì‹œ ì‹œê°„ë‚  ë•Œ ë³´ê¸°.

As mentioned in Section 3.4, accelerating division is more challenging than multiplication. In addition to SRT, another technique to leverage a fast multiplier is **Newtonâ€™s iteration**, where division is recast as finding the zero of a function to produce the reciprocal 1/c, which is then multiplied by the other operand. Iteration techniques cannot be rounded properly without calculating many extra bits. A TI chip solved this problem by calculating an extra-precise reciprocal.

ë¶€ë™ì†Œìˆ˜ì ì˜ ë°˜ì˜¬ë¦¼ì„ ì •í™•í•˜ê²Œ í•˜ë ¤ë©´ ì¶”ê°€ì ì¸ ë¹„íŠ¸ë¥¼ ê´€ë¦¬í•˜ëŠ” í•˜ë“œì›¨ì–´ê°€ í•„ìš”í•˜ë‹¤. (ì „ì— ë‚´ê°€ ê¶ê¸ˆí•´í•œ ê²ƒì²˜ëŸ¼ ë¹„íŠ¸ ìë¥´ëŠ”ê±¸ ê³±ì…ˆ ì§í›„ì— í•˜ë©´ ë°˜ì˜¬ë¦¼ì˜ ì—¬ì§€ê°€ ì—†ë‹¤? <- ê³±ì…ˆ ë‚´ìš© / ë§ì…ˆ ë‚´ìš© ->) ë”°ë¼ì„œ guardì™€ round ë‘ ë¹„íŠ¸ë¥¼ ì¤‘ê°„ ë§ì…ˆ ê³¼ì •ì—ì„œ ìœ ì§€í•œë‹¤.

[Here is an example of why you need a Guard bit, in addition to the Round and Sticky bits.](https://pages.cs.wisc.edu/~david/courses/cs552/S12/handouts/guardbits.pdf)

> **guard**: The first of two extra bits kept on the right during intermediate calculations of floating-point numbers; used to improve rounding accurary.

> **round**: Method to make the intermediate floating-point result fit the floating-point format; the goal is typically to find the nearest number that can be represented in the format. It is also the name of the second of two extra bits kept on the right during intermediate floating-point calculations, which improves rounding accuracy.

> **units in the last place (ulp)** The number of bits in error in the least significant bits of the significand between the actual number and the number that can be represented.

If a number were off by 2 in the least significant bits, it would be called off by 2 ulps. Provided there are no overflow, underflow, or invalid operation exceptions, IEEE 754 guarantees that the computer uses the number that is within **one-half ulp**.

ê³±ì…ˆì€ ì¶”ê°€ ë¹„íŠ¸ê°€ ê¼­ 2ê°œ í•„ìš”í•˜ë‹¤. A binary product may have one leading 0 bit; hence, the normalizing step must shift the product one bit left. This shifts the guard digit into the least significant bit of the product, leaving the round bit to help accurately round the product.

> **sticky bit**: A bit used in rounding in addition to guard and round that is set whenever there are nonzero bits to the right of the round bit.

The goal of the extra rounding bits is to allow the computer to get the same results as if the intermediate results were calculated to infinite precision and then rounded. To support this goal and round to the nearest even, the standard has a third bit in addition to guard and round; it is set whenever there are nonzero bits to the right of the round bit. This sticky bit allows the computer to see the difference between 0.50 ... 00ten and 0.50 ... 01ten when rounding.

RISC-V, MIPS-64, PowerPC, AMD SSE5, and Intel AVX architectures all provide a single instruction that does a multiply and add on three registers: a = a + (b Ã— c). Obviously, this instruction allows potentially higher floating-point performance for this common operation. Equally important is that instead of performing two roundingsâ€”after the multiply and then after the addâ€”which would happen with separate instructions, the multiply add instruction can perform a **single rounding** after the add. A single rounding step increases the precision of multiply add. Such operations with a single rounding are called **fused multiply add**. It was added to the revised IEEE 754-2008 standard

> **fused multiply add**: A floating-point instruction that performs both a multiply and an add, but rounds only once after the add.

ë°˜ì˜¬ë¦¼ì€ ë‚˜ì¤‘ì— ì†ìœ¼ë¡œë„ í•´ë³´ê¸°,,

**The BIG Picture**: Bit patterns have no inherent meaning. They may represent signed integers, unsigned integers, floating-point numbers, instructions, character strings, and so on. What is represented depends on the instruction that operates on the bits in the word. The major difference between computer numbers and numbers in the real world is that computer numbers have limited size and hence limited precision; itâ€™s possible to calculate a number too big or too small to be represented in a computer word. Programmers must remember these limits and write programs accordingly.

To accommodate comparisons that may include NaNs, the standard **includes ordered and unordered as options for compares**. RISC-V does not provide instructions for unordered comparisons, but a careful sequence of ordered comparisons has the same effect. (Java does not support unordered compares.)

[What does ordered / unordered comparison mean?](https://stackoverflow.com/questions/8627331/what-does-ordered-unordered-comparison-mean)

In an attempt to squeeze every bit of precision from a floating-point operation, the standard allows some numbers to be represented in unnormalized form. Rather than having a gap between 0 and the smallest normalized number, IEEE allows **denormalized numbers** (also known as denorms or subnormals). They have the same exponent as zero but a nonzero fraction. They allow a number to degrade in significance until it becomes 0, called gradual underflow. For example, the smallest positive single precision normalized number is

1.0000 0000 0000 0000 0000 0000 \* 2^-126

but the smallest single precision denormalized number is

0.0000 0000 0000 0000 0000 001 \* 2^-126 or 1.0 \* 2^-129

For double precision, the denorm gap goes from 1.0 Ã— 2âˆ’1022 to 1.0 Ã— 2âˆ’1074.

The possibility of an occasional unnormalized operand has given headaches to floating-point designers who are trying to build fast floating-point units. Hence, many computers cause an exception if an operand is denormalized, letting software complete the operation. Although software implementations are perfectly valid, their lower performance has lessened the popularity of denorms in portable floating-point software. Moreover, if programmers do not expect denorms, their programs may surprise them.

**Check Yourself**: 3. ë” ì •í™•íˆ í’€ì–´ë³´ê¸°.

## 4. The Processor

1-4

### 1. Introduction

lwëŠ” ë ˆì§€ìŠ¤í„° 1ê°œ ì½ì§€ë§Œ ë‚˜ë¨¸ì§€ ëŒ€ë¶€ë¶„ì€ 2ê°œ ì½ëŠ”ë‹¤.

ëª¨ë“  instruction classë“¤ì´ ë ˆì§€ìŠ¤í„°ë¥¼ ì½ì€ í›„ ALUë¥¼ ì‚¬ìš©í•œë‹¤.

A **control unit**, which has the instruction as an input, is used to determine how to set the control lines for the functional units and two of the multiplexors.

Figure 4.2 ê·¸ë ¤ë³´ê¸°.

**Check Yourself**: datapath, control, memory. (input output ì—†ìŒ.)

**Check Yourself**: false. **false. Edge-triggered state elements make simultaneous reading and writing both possible and unambiguous.**

### 2. Logic Design Conventions

Furthermore, if we saved and restored the state elements, it would be as if the computer had never lost power. Thus, these state elements completely characterize the computer.

...our RISC-V implementation uses two other types of state elements: memories and registers

Logic components that contain state are also called sequential. ìˆœì°¨ íšŒë¡œê°€ ì¡°í•© íšŒë¡œë¥¼ í¬í•¨í•  ìˆ˜ ìˆëŠ” ëŠë‚Œì¸ë“¯?

ìˆœì°¨ íšŒë¡œì˜ ì¶œë ¥ì€ ì…ë ¥ê³¼ ë‚´ë¶€ ìƒíƒœì— ì—°ê´€ë˜ì–´ìˆë‹¤. Mealy machineìœ¼ë¡œ ìƒê°í•˜ëŠ”ê±´ê°€? ë ˆì§€ìŠ¤í„°ë¥¼ ìƒê°í•´ë³´ë©´ ì…ë ¥í•œ ë ˆì§€ìŠ¤í„° ë²ˆí˜¸ì™€ ì´ì „ì— ë ˆì§€ìŠ¤í„°ì— ì“°ì¸ ê°’ê³¼ ê´€ë ¨ë˜ì–´ìˆë‹¤.

> **control signal**: A signal used for multiplexor selectio or for directing the operation of a functional unit; contrasts with a data signal, which contains information that is operated on by a functional unit.

**asserted**ëŠ” logically high or true, **deasserted**ëŠ” logically low or false.

### 3. Building a Datapath

> **datapath element**: A unit used to operate on or hold data within a processor.

Two state elements are needed to store and access instructions. Instruction memory + PC. **Instruction memoryëŠ” read accessë§Œ ì œê³µëœë‹¤ë©´ combinational logicìœ¼ë¡œ ê°„ì£¼í•œë‹¤.** í”„ë¡œê·¸ë¨ì„ loadí•  ë–„ëŠ” ì“°ê¸° ì‘ì—…ì´ í•„ìš”í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ìƒëµ. PCëŠ” ë§¤ë²ˆ writeë˜ê¸°ì— write control signalì´ í•„ìš”í•˜ì§€ ì•Šë‹¤.

The register file always outputs the contents of whatever register numbers are on the Read register inputs. Writes, however, are controlled by the write control signal, which must be asserted for a write to occur at the clock edge.

Since writes to the register file are edge-triggered, our design can legally read and write the same register within a clock cycle: the read will get the value written in an earlier clock cycle, while the value written will be available to a read in a subsequent clock cycle.

> **branch target address**: The address specified in a branch, which becomes the new PC if the branch is taken. In the RISC-V architecture, the branch target is given by the sum of the offset field of the instruction and the **address of the branch**.

Offset fieldëŠ” half word offsetìœ¼ë¡œ ì‹¤ì œ ê°’ì„ ì–»ìœ¼ë ¤ë©´ shift right 1 í•´ì•¼í•œë‹¤.

Data memory unitì— MemWrite ìˆëŠ”ê±´ ì•Œì•˜ëŠ”ë° MemReadë„ ìˆë‹¤. CLKì€ ì•ˆë¨¹ë‚˜? The memory unit needs a read signal, since, unlike the register file, reading the value of an invalid address can cause problems. ì–¸ì  ê°€ ì‚´í´ë³¸ ê²ƒì²˜ëŸ¼ MemRead ì—†ìœ¼ë©´ segmentation faultë“±ì´ ì¼ì–´ë‚  ìˆ˜ ìˆë‹¤.

The immediate generation unit (ImmGen) has a 32-bit instruction as input that selects a 12-bit field for load, store, and branch if equal that is sign-extended into a 32-bit result appearing on the output.

This simplest datapath will attempt to execute all instructions in one clock cycle. Thus, that no datapath resource can be used more than once per instruction, so any element needed more than once must be **duplicated**. We therefore need a memory for instructions separate from one for data?

The immediate generation logic must choose between sign-extending a 12-bit field in instruction bits 31:20 for load instructions, bits 31:25 and 11:7 for store instructions, or bits 31, 7, 30:25, and 11:8 for the conditional branch. Since the input is all 32 bits of the instruction, it can use the opcode bits of the instruction to select the proper field. **RISC-V opcode bit 6 happens to be 0 for data transfer instructions and 1 for conditional branches, and RISC-V opcode bit 5 happens to be 0 for load instructions and 1 for store instructions. Thus, bits 5 and 6 can control a 3:1 multiplexor inside the immediate generation logic that selects the appropriate 12-bit field for load, store, and conditional branch instructions**.

**Check Yourself**: I-a, II-c. the processor operates in one clock cycle and cannot use a (single- ported) memory for two different accesses within that clock cycle.

### 4. A Simple Implementation Scheme

ì—¬ê¸°ì„œ ALUëŠ” opcodeê³¼ 2-bit signalì„ control inputìœ¼ë¡œ ë°›ëŠ”ë‹¤??

We can generate the 4-bit ALU control input using a small control unit that has as inputs the **funct7** and **funct3** fields of the instruction and a 2-bit control field, which we call **ALUOp**??

This style of using multiple levels of decodingâ€”that is, the main control unit generates the ALUOp bits, which then are used as input to the ALU control that generates the actual signals to control the ALU unitâ€”is a common implementation technique. Using multiple levels of control can reduce the size of the main control unit. Using several smaller control units may also potentially reduce the latency of the control unit. Such optimizations are important, since the latency of the control unit is often a critical factor in determining the clock cycle time.

Funct fields are used only when the ALUOp bits equal 10.

note that the only bits with different values for the four R-format instructions are bits 30, 14, 13, and 12. Thus, we only need these four funct field bits as input for ALU control instead of all 10.

SBì—ì„œëŠ” immediateê°€ shiftëœë‹¤ëŠ” íŠ¹ì§•ì´ ìˆë‹¤.

RISC-V has two formats where all the fields are the same size and are immediates as in two other formatsâ€”SB versus S and UJ versus U.

Once again, RISC-V architects designed odd-looking but efficient formats, simplifying **18** 1-bit multiplexors.

The ALU control block has also been added, which depends on the funct3 field and part of the funct7 field.

The control unit can set all but one of the control signals based solely on the opcode and funct fields of the instruction. The PCSrc control line is the exception.

ALU controlì— inst[30, 14-12]ì™€ ALUOpê°€ ë“¤ì–´ê°€ì„œ ALU ì…ë ¥ì¸ 4ë¹„íŠ¸ ê°’ì´ ë‚˜ì˜¨ë‹¤. ALUOpëŠ” ì–´ë””ì„œ ë‚œê±°ì•¼?? Figure 4.21 ë³´ë‹ˆê¹Œ controlì—ì„œ ALUOpê°€ ë‚˜ì˜¤ê³  ì–˜ë¥¼ ë‹¤ì‹œ ALU controlì— ë„£ëŠ”ë‹¤. Controlì—ì„œëŠ” inst[6-0]ë§Œ ë‹´ë‹¹í•˜ëŠ” ì‹ìœ¼ë¡œ ì—­í•  ë¶„ë‹´ì„ í•œ ë“¯.

Read register 1ì€ ë¬´ì¡°ê±´ 19-15, read register 2ëŠ” ë¬´ì¡°ê±´ 24-20, write registerëŠ” 11-7ì´ë¼ì„œ í–‰ë³µí•˜ë‹¤. Immgenì€ ë‹¤ë¨¹ìŒ. êµ³ì´? opcode ë¹¼ê³  ì¤˜ë„ ë˜ê¸´ ë˜ëŠ”ê±° ì•„ë‹Œê°€.

RegWrite, ALUSrc, PCSrc, MemRead, MemWrite, MemtoReg.

The input to the control unit is the 7-bit opcode field from the instruction. The outputs of the control unit consist of two 1-bit signals that are used to control multiplexors (ALUSrc and MemtoReg), three signals for controlling reads and writes in the register file and data memory (RegWrite, MemRead, and MemWrite), a 1-bit signal used in determining whether to possibly branch (Branch), and a 2-bit control signal for the ALU (ALUOp).

Mem-Writeì™€ Mem-Read ì¤‘ í•˜ë‚˜ë§Œ 1ì´ë‹¤.

ê°€ì¥ ê¸´ ê²½ë¡œë¥¼ ê°€ì§„ ëª…ë ¹ì–´ëŠ” most likely a load instruction, which uses five functional units in series: the instruction memory, the register file, the ALU, the data memory, and the register file.

**Check Yourself**: Can any control signal output in the figure be replaced by the inverse of another? (Hint: take into account the donâ€™t cares.) If so, can you use one signal for the other without adding an inverter? -> MemtoRegì™€ ALUOp1. MemtoReg MUXì˜ 1ê³¼ 0ì„ ë°˜ëŒ€ë¡œ í•œë‹¤. + Branch and ALUOp0, ALUSrc and MemtoReg.

ì—¬ê¸°ê¹Œì§€ ì¤‘ê°„ê³ ì‚¬ - - - - - - - - - - -

6-10

## 5. Large and Fast: Exploiting Memory Hierarchy

1-4, 7-8

## Appendix A

ì¤‘ê°„ê³ ì‚¬ reading assignment ì¼ë¶€

ìƒê°ë³´ë‹¤ ê¸¸ì–´ì„œ pass.

## í€´ì¦ˆ

ARM is one of the least RISCy, having a load-multiple instruction and complex addressing modes. [ARM vs RISC and x86 vs CISC](https://stackoverflow.com/questions/72962725/arm-vs-risc-and-x86-vs-cisc)

[Why does Intel hide internal RISC core in their processors?](https://stackoverflow.com/questions/5806589/why-does-intel-hide-internal-risc-core-in-their-processors)

## ì¤‘ê°„ê³ ì‚¬ practice.questions

1. What does computer architecture specify?

Computer architecture is a specification detailing how a set of software and hardware technology standards interact to form a computer system or platform. In short, computer architecture refers to how a computer system is designed and what technologies it is compatible with.

2. What are the "Seven Great Ideas in Computer Architecture"?

- Use Abstraction to Simplify Design
- Make the Common Case Fast
- Performance via Parallelism
- Performance via Prediction
- Hierarchy of Memories
- Dependability via Redunduncy

3. What are the basic components of (almost) every computer?

Datapath, control, input, output, memory

4. The compiler performs what job?

A program that translates high-level language statements into assembly language statements.

5. The assembler performs what job?

A program that translates a symbolic version of instructions into the binary version.

6. What is the yield of a chip manufacturing process?

The percentage of good dies from the total number of dies on the wafer.

7. Explain the difference between little- and big- endian memory systems.

A big-endian system stores the most significant byte of a word at the smallest memory address and the least significant byte at the largest. A little-endian system, in contrast, stores the least-significant byte at the smallest address.

8. pass

9. pass

10. ~x + x = 111...111 = -1

11.

value = value >> 31; return !value | !~value

12. overflow

13. 232B

14. ???

15. a0-a7, use stack if more than 8.

16. í•¨ìˆ˜ë¡œ jumpí•  ë•Œ ë‹¤ìŒ ëª…ë ¹ì–´ì˜ ì£¼ì†Œë¥¼ return address registerì— ì €ì¥í•œë‹¤. ì—¬ê¸° ê°’ìœ¼ë¡œ ë˜ëŒì•„ê°„ë‹¤.

17. ì „ë¶€ ì €ì¥ ì•ˆí•´ë„ ë˜ê²Œ

18. local variable, array, struct, spilled register...

19. a + sizeof(int) _ x _ y + sizeof(int) \* y

20. count number of ld operation

21. https://stackoverflow.com/questions/56761591/how-do-i-organize-members-in-a-struct-to-waste-the-least-space-on-alignment

22. What is a combinational circuit?

A logic system whose blocks do not contain memory and hence compute the same output given the same input.

23. What is a sequential circuit?

A group of logic elements that contain memory and hence whose value depends on the inputs as well as the current contents of the memory.

24. Why can we not clock processors as fast as we want?

The longest possible path in the processor determines the clock cycle. í•œ ì‚¬ì´í´ ë‚´ì— memoryì— ì €ì¥ë  ê°’ì´ ì¤€ë¹„ë˜ì–´ì•¼í•œë‹¤.

25. ê·¸ë¦¼ ê·¸ë¦¼

26. ë¶€ë™ì†Œìˆ˜ì ì€ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ìë¦¿ìˆ˜(fraction)ì— í•œê³„ê°€ ìˆë‹¤. ë”°ë¼ì„œ ì†Œìˆ˜ì  ìœ„ì™€ ì•„ë˜ ëª¨ë‘ ë„ˆë¬´ ë§ìœ¼ë©´ ì†Œìˆ˜ì ìª½ì´ ì˜ë¦¼??

27. What does the Instructino Set Architecture of a processor architecture define?

Memory addressing, Type and size of operands, Operations, Instruction encoding

28. Explain the differences between Intel and RISC-V processors.

https://cs.stanford.edu/people/eroberts/courses/soco/projects/risc/risccisc/

## ê¸°íƒ€

https://cs.stanford.edu/people/eroberts/courses/soco/projects/risc/risccisc/

https://www.forrestthewoods.com/blog/perfect_prevention_of_int_overflows/

https://stackoverflow.com/questions/59693334/why-ra-is-caller-saved-in-risc-v

https://stackoverflow.com/questions/9772348/get-absolute-value-without-using-abs-function-nor-if-statement